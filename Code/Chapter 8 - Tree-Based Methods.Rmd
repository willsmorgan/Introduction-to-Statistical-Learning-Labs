---
title: "Chapter 8 Lab: Decision Trees"
output: rmarkdown::github_document
---

``` {r setup, include = FALSE}
rm(list = ls(all = TRUE))

libs <- c("tidyverse", "tree", "ISLR", 'stringr', 'MASS', 'randomForest', 'gbm')
invisible(lapply(libs, library, character.only = TRUE))
```
##### Lab Topics:

* Classification and Regression Trees with Pruning

* Random Forests and Bagging

* Boosting

***

### 8.3.1: Fitting Classification Trees
The `tree` library is used to construct classification and regression trees. We'll be
using the `Carseats` data from the `ISLR` library. Instead of using the continuous
variable `Sales` we will create our own dummy variable `high` to indicate sales greater
than 8

```{r data, message = FALSE}
data <- Carseats %>%
  mutate(high = factor(if_else(Sales > 8, 1, 0)))

names(data) <-  str_to_lower(names(data))
```

`tree::tree()` will allow us to fit the classification tree to predict the 
variable we just created. The first argument is the only thing we need to 
worry about at this point. We simply need to insert the formula that will
be used to make our predictions, similar to `lm()`. There are a variety of packages
available to perform recursive partitioning, like `rpart` and `ctree`. If you 
would like to learn more, you can visit [this](https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/)
blog post about available methods.

```{r creating the tree}
# Set up initial tree
tree <- tree(high ~ . -sales, data)
```

The output from this tree is stored in a list, which we can
summarize in order to view the variables that are used in internal nodes,
the number of terminal nodes, and the training error rate. You can also just
call on the object itself to get the entire representation of the tree. Terminal
nodes are denoted with asterisks. To save space, we exclude that from this document

```{r}
summary(tree)
```

We know that trees can be easily be visually interpreted, so let's 
check out what our test case looks like. Be wary of doing any plotting
because the tree can easily grow out of control and make the graph impossible
to read. We can see that in the following plot of our current results:

```{r, echo = FALSE}
plot(tree)
text(tree, pretty = 0, cex = .65, digits = 1)
```

In order to evaluate a classification tree we need to use training and testing
sets. Let's now repeat what we did above, this time including the calculation
for the test error rate.

```{r}
# Define our training/testing sets
set.seed(2)
train <- sample_n(data, 200)
test <-  setdiff(data, train)

# Run the recursive partioning algorithm
ttree <- tree(high ~. -sales, data = train)

# Make predictions and display confusion matrix
test_predictions <- predict(ttree, test, type = 'class')
table(test_predictions, test$high)

(86 + 57) / 200

```

We can now add on another layer of complexity by pruning our results. Recall that 
unpruned trees are prone to overfitting the data, so our method will be to watch
variation in test error rates as we increase the penalty in the number of terminal 
nodes. To refresh your memory, we summarize __Algorithm 8.1__  below:  

##### __Algorithm 8.1: Pruning Trees__

1. Grow your original tree $T_0$ using your training data

2. As a function of $\alpha$ (the penalty parameter), define a sequence of best
subtrees 

3. Use K-fold cross-validation to find the $\alpha$ that minimizes the
average mean squared prediction error of the $k$th fold of the training data

4. Find the best subtree from Step 2 using the $\alpha$ found in the previous step


Luckily, `tree::cv.tree` contains will be doing most of the work for us. It will
perform the cross-validation required to determine the optimal tree size. It also
allows us to choose the function by which the tree is pruned. In this case, we pruning
will be guided by the classification error rate.

```{r}
set.seed(3)
cv_tree <- cv.tree(ttree, FUN = prune.misclass)
cv_tree
```

Most important in these results is `$dev`, which corresponds to the cross-validation
error in each instance. We can see that the smallest value occurs when there 
are 9 terminal nodes. Let's take a quick look at how the error varies in the our 
number of terminal nodes:

```{r, echo = FALSE}
ggplot(data = data.frame(cv_tree$size, cv_tree$dev),
  aes(x = cv_tree$size, y = cv_tree$dev)) +
  geom_line(color = "darkblue") +
  labs(x = "Tree Size", y = "Number of Errors", title = "CV Error by Tree Size") +
  theme(plot.title = element_text(hjust = .5))
```

Now that we know how exactly how many terminal nodes we want, we prune our tree
with `prune.misclass()` to obtain the optimal tree. Then, check to see if this
tree performs any better on the testing set than the base tree $T_0$ did. 

```{r}
pruned <- prune.misclass(ttree, best = 9)

test_predictions <- predict(pruned, data = test, type = 'class')
table(test_predictions, test$high)

(94 + 60) / 200
```

***

### 8.3.2 Fitting Regression Trees

Not much changes in terms of the code when we switch to regression trees, so this
section will pretty much be a recap of the previous one, just using different data.
We pull the `Boston` data set from the `MASS` library for this exercise. 

```{r}
Boston <- MASS::Boston
set.seed(1)
train <- sample_frac(Boston, .5) 
test <-  setdiff(Boston, train)

tree_train <- tree(medv ~ ., data = train)
summary(tree_train)
```

```{r}
plot(tree_train)
text(tree_train, pretty = 0, cex = .65)
```

As you can see `lstat < 9.715` is the first partition in this tree. The variable measures
the percentage of individuals with lower socioeconomic status in the immediate area. Based 
off of the terminal nodes stemming from the left side of the tree, this suggests that 
higher socioeconomic geographic areas end up with much larger median house prices.  

We can now move on to see if pruning will increase the performance of this tree

```{r}
cv_tree <- cv.tree(tree_train)

# Get an idea of change in error by changing tree size
ggplot(data = data.frame(cv_tree$size, cv_tree$dev),
  aes(x = cv_tree$size, y = cv_tree$dev)) +
  geom_line(color = "darkblue") +
  labs(x = "Tree Size", y = "Number of Errors", title = "CV Error by Tree Size") +
  theme(plot.title = element_text(hjust = .5))

# Predict, plot, and calculate MSE
yhat <- predict(tree_train, newdata = test)
test_outcomes <- test$medv

plot(yhat, test_outcomes)

mean((yhat - test_outcomes)^2)

```

*** 

### 8.3.3: Bagging and Random Forests

We'll be using the same data from the previous section and the `randomForest` package
to help us accomplish some simple examples. We begin with a bagging example, 
where all predictors are used in each split. 

```{r}
set.seed(1)
train <- sample_frac(Boston, .5) 
test <-  setdiff(Boston, train)

# Set up the randomForest for the bagging case (all vars included)
bag <- randomForest(medv ~ ., data = train,
                    mtry = 13, importance = TRUE) 
bag

# Calculate MSE of the testing set for the bagged regression tree
yhat <- predict(bag, test)
mean((yhat - test$medv)^2)
```

Compare the MSE of the bagged random forest to the optimally-pruned single tree
found in 8.3.2 - it's much lower. We manually changed the amount of variables at each
split in the above bagging example, but we might achieve even better results using a more
general random forest. By default, `randomForest` uses $p/3$ variables when building
a forest of regression trees and $\sqrt p$ for classification trees. In the following
example, we will use `mtry=6` ($m \approx p/2$). 

```{r}
forest <- randomForest(medv ~., data = train, mtry = 6, importance = TRUE)

yhat <- predict(forest, test)
mean((yhat - test$medv)^2)
```

We find that this approach worked - our MSE is now reduced to 11.37, lower than
the previous two methods we tried.

Once we find an adequate forest, we can check out how important each variable is
using `importance()`.

```{r}
importance(forest)
```

The first column represents the mean decrease in accuracy of the prediction when
the variable is removed from the model, and the second column is a measure of the
total decrease in node impurity resulting from splits over that variable (averaged
over all of the trees)

`randomForest::varImpPlot()` plots these importance measures for us

```{r}
varImpPlot(forest)
```

***

### 8.3.4 Boosting

We'll be using the `gbm` package to help us fit boosted regression trees to the 
`Boston` data set, which you should be familiar with by now. 

```{r}
set.seed(1)
train <- sample_frac(Boston, .5) 
test <-  setdiff(Boston, train)

boosted <- gbm(medv ~ ., train, distribution = 'gaussian', # regression => distr = 'gaussian'
               n.trees = 5000, interaction.depth = 4)

# Summarize and produce a quick plot to highlight importance of variables
summary(boosted)
```

Let's now plot the marginal effect of these top two variables, `lstat` and `rm`

```{r}
par(mfrow = c(1,2))
plot(boosted, i = 'rm')
plot(boosted, i = 'lstat')
```

Alright - this just confirmed what we should have already been expecting: median 
house values are decreasing in `lstat` and increasing in `rm`

Let's now test how well this boosted regression tree performs on the testing data

```{r}
yhat <- predict(boosted, newdata = test, n.trees = 5000)
mean((yhat - test$medv)^2)
```

Not amazing, but not bad. The boosted model performed just around the same as the 
random forests and superior to the bagging model, but we might be able to squeeze 
out some extra performance by changing the shrinkage parameter $\lambda$. The default
value is .001, but let's bump it up to $\lambda = .2$

```{r}
boosted <- gbm(medv ~., train, distribution = 'gaussian',
               n.trees = 5000, interaction.depth = 4,
               shrinkage = .2, verbose = F)
yhat <- predict(boosted, newdata = test, n.trees = 5000)
mean((yhat - test$medv)^2)
```

Changing the shrinkage parameter actually made a difference - we're now just slightly
under what we got from the previous model where it was equal to .001